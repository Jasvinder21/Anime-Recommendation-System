{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91971\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: requests in c:\\users\\91971\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://myanimelist.net/topanime.php\"\n",
    "response = requests.get(url)\n",
    "#html_content = response.text\n",
    "\n",
    "\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Now you can work with the parsed content (soup object)\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#soup = BeautifulSoup(response, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_names =[]\n",
    "anime_details = []\n",
    "scores = []\n",
    "links = []\n",
    "descriptions = []\n",
    "english_title= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://myanimelist.net/topanime.php?limit=850\n",
      "Scraping URL: https://myanimelist.net/topanime.php?limit=900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Base URL of the page\n",
    "base_url = 'https://myanimelist.net/topanime.php'\n",
    "\n",
    "\n",
    "for page in range(17, 19):  # You can adjust the range to scrape more pages\n",
    "    # Construct the URL for the current page\n",
    "    url = f\"{base_url}?limit={page * 50}\"  # MyAnimeList paginates every 50 entries\n",
    "    print(f\"Scraping URL: {url}\")\n",
    "    response = requests.get(url).text\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    \n",
    "    # Find all anime elements in the table rows\n",
    "    anime_elements = soup.find_all(\"tr\", class_=\"ranking-list\")\n",
    "    \n",
    "    for anime_element in anime_elements:\n",
    "        # Get the title\n",
    "         # Get the title and link\n",
    "        title_element = anime_element.find(\"h3\", class_=\"anime_ranking_h3\").find(\"a\")\n",
    "        link = title_element.get(\"href\") if title_element else None\n",
    "        title = title_element.text.strip() if title_element else \"Title not found\"\n",
    "            \n",
    "        \n",
    "        anime_d = anime_element.find(\"div\", class_=\"information di-ib mt4\")\n",
    "        score = anime_element.find(\"div\", class_=\"js-top-ranking-score-col di-ib al\")\n",
    "        \n",
    "        # Extract title if available\n",
    "        if title_element:\n",
    "            anime_names.append(title_element.text.strip())\n",
    "        \n",
    "        # Extract anime details if available\n",
    "        if anime_d:\n",
    "            anime_details.append(anime_d.get_text(separator=\"\\n\").strip())\n",
    "        \n",
    "        if link:\n",
    "            links.append(link)\n",
    "        \n",
    "        # Extract score if available\n",
    "        if score:\n",
    "            scores.append(score.text.strip())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching description for: - https://myanimelist.net/anime/14719/JoJo_no_Kimyou_na_Bouken_TV\n",
      "Fetching description for: - https://myanimelist.net/anime/3572/Macross_F\n",
      "Fetching description for: - https://myanimelist.net/anime/36563/Megalo_Box\n",
      "Fetching description for: - https://myanimelist.net/anime/780/Meitantei_Conan_Movie_02__14-banme_no_Target\n",
      "Fetching description for: - https://myanimelist.net/anime/31490/One_Piece_Film__Gold\n",
      "Fetching description for: - https://myanimelist.net/anime/38201/Osomatsu-san_Movie\n",
      "Fetching description for: - https://myanimelist.net/anime/957/Saiunkoku_Monogatari\n",
      "Fetching description for: - https://myanimelist.net/anime/1594/Jigoku_Shoujo_Futakomori\n",
      "Fetching description for: - https://myanimelist.net/anime/52168/Kidou_Senshi_Gundam__Suisei_no_Majo_-_Prologue\n",
      "Fetching description for: - https://myanimelist.net/anime/16782/Kotonoha_no_Niwa\n",
      "Fetching description for: - https://myanimelist.net/anime/55855/Kuroshitsuji__Kishuku_Gakkou-hen\n",
      "Fetching description for: - https://myanimelist.net/anime/30952/New_Initial_D_Movie__Legend_3_-_Mugen\n",
      "Fetching description for: - https://myanimelist.net/anime/32547/Non_Non_Biyori_Repeat__Hotaru_ga_Tanoshinda\n",
      "Fetching description for: - https://myanimelist.net/anime/2159/Ookiku_Furikabutte\n",
      "Fetching description for: - https://myanimelist.net/anime/33926/Quanzhi_Gaoshou\n",
      "Fetching description for: - https://myanimelist.net/anime/11813/Shijou_Saikyou_no_Deshi_Kenichi_OVA\n",
      "Fetching description for: - https://myanimelist.net/anime/2752/Tennis_no_Oujisama__Zenkoku_Taikai-hen_-_Semifinal\n",
      "Fetching description for: - https://myanimelist.net/anime/54790/Undead_Girl_Murder_Farce\n",
      "Fetching description for: - https://myanimelist.net/anime/58930/Yuanshen__Weixing_zhi_Lu\n",
      "Fetching description for: - https://myanimelist.net/anime/12113/Berserk__Ougon_Jidai-hen_II_-_Doldrey_Kouryaku\n",
      "Fetching description for: - https://myanimelist.net/anime/38408/Boku_no_Hero_Academia_4th_Season\n",
      "Fetching description for: - https://myanimelist.net/anime/10012/Carnival_Phantasm\n",
      "Fetching description for: - https://myanimelist.net/anime/14353/Death_Billiards\n",
      "Fetching description for: - https://myanimelist.net/anime/2924/ef__A_Tale_of_Memories\n",
      "Fetching description for: - https://myanimelist.net/anime/54968/Giji_Harem\n",
      "Fetching description for: - https://myanimelist.net/anime/3014/Ginga_Eiyuu_Densetsu__Waga_Yuku_wa_Hoshi_no_Taikai\n",
      "Fetching description for: - https://myanimelist.net/anime/934/Higurashi_no_Naku_Koro_ni\n",
      "Fetching description for: - https://myanimelist.net/anime/2828/Ie_Naki_Ko\n",
      "Fetching description for: - https://myanimelist.net/anime/249/InuYasha\n",
      "Fetching description for: - https://myanimelist.net/anime/54846/Aishang_Ta_de_Liyou\n",
      "Fetching description for: - https://myanimelist.net/anime/51680/Cool_Doji_Danshi\n",
      "Fetching description for: - https://myanimelist.net/anime/38436/Doupo_Cangqiong_3rd_Season\n",
      "Fetching description for: - https://myanimelist.net/anime/30415/Highâ˜†Speed_Movie__Free_Starting_Days\n",
      "Fetching description for: - https://myanimelist.net/anime/5680/K-On\n",
      "Fetching description for: - https://myanimelist.net/anime/10080/Kami_nomi_zo_Shiru_Sekai_II\n",
      "Fetching description for: - https://myanimelist.net/anime/56704/Kengan_Ashura_Season_2_Part_2\n",
      "Fetching description for: - https://myanimelist.net/anime/49828/Kidou_Senshi_Gundam__Suisei_no_Majo\n",
      "Fetching description for: - https://myanimelist.net/anime/56609/Kizumonogatari__Koyomi_Vamp\n",
      "Fetching description for: - https://myanimelist.net/anime/34021/Lupin_the_IIIrd__Chikemuri_no_Ishikawa_Goemon\n",
      "Fetching description for: - https://myanimelist.net/anime/25517/Magic_Kaito_1412\n",
      "Fetching description for: - https://myanimelist.net/anime/4985/Mahou_Shoujo_Lyrical_Nanoha__The_Movie_1st\n",
      "Fetching description for: - https://myanimelist.net/anime/779/Meitantei_Conan_Movie_01__Tokei_Jikake_no_Matenrou\n",
      "Fetching description for: - https://myanimelist.net/anime/38770/Meitantei_Conan_Movie_23__Konjou_no_Fist\n",
      "Fetching description for: - https://myanimelist.net/anime/36215/One_Piece__Episode_of_East_Blue_-_Luffy_to_4-nin_no_Nakama_no_Daibouken\n",
      "Fetching description for: - https://myanimelist.net/anime/6007/Ookami_to_Koushinryou_II__Ookami_to_Kohakuiro_no_Yuuutsu\n",
      "Fetching description for: - https://myanimelist.net/anime/322/Paradise_Kiss\n",
      "Fetching description for: - https://myanimelist.net/anime/2158/Terra_e_TV\n",
      "Fetching description for: - https://myanimelist.net/anime/41389/Tonikaku_Kawaii\n",
      "Fetching description for: - https://myanimelist.net/anime/54829/Urusei_Yatsura_2022_2nd_Season\n",
      "Fetching description for: - https://myanimelist.net/anime/49570/Wu_Dong_Qian_Kun_3rd_Season\n",
      "Fetching description for: - https://myanimelist.net/anime/37936/Wu_Shan_Wu_Xing_2020\n",
      "Fetching description for: - https://myanimelist.net/anime/6377/Zan_Sayonara_Zetsubou_Sensei\n",
      "Fetching description for: - https://myanimelist.net/anime/8063/Sekaiichi_Hatsukoi_OVA\n",
      "Fetching description for: - https://myanimelist.net/anime/3588/Soul_Eater\n",
      "Fetching description for: - https://myanimelist.net/anime/22/Tennis_no_Oujisama\n",
      "Fetching description for: - https://myanimelist.net/anime/949/Top_wo_Nerae_Gunbuster\n",
      "Fetching description for: - https://myanimelist.net/anime/35507/Youkoso_Jitsuryoku_Shijou_Shugi_no_Kyoushitsu_e\n",
      "Fetching description for: - https://myanimelist.net/anime/23225/Yuru_Yuri_Nachuyachumi\n",
      "Fetching description for: - https://myanimelist.net/anime/58509/Zhu_Xian_2nd_Season\n",
      "Fetching description for: - https://myanimelist.net/anime/31964/Boku_no_Hero_Academia\n",
      "Fetching description for: - https://myanimelist.net/anime/37435/Carole___Tuesday\n",
      "Fetching description for: - https://myanimelist.net/anime/2471/Doraemon_1979\n",
      "Fetching description for: - https://myanimelist.net/anime/7645/Heartcatch_Precure\n",
      "Fetching description for: - https://myanimelist.net/anime/35082/Hibike_Euphonium_Movie_2__Todoketai_Melody\n",
      "Fetching description for: - https://myanimelist.net/anime/37029/Hoozuki_no_Reitetsu_2nd_Season__Sono_Ni\n",
      "Fetching description for: - https://myanimelist.net/anime/54118/IDOLiSH7_Movie__LIVE_4bit_-_BEYOND_THE_PERiOD\n",
      "Fetching description for: - https://myanimelist.net/anime/9734/K-On__Keikaku\n",
      "Fetching description for: - https://myanimelist.net/anime/18429/Lupin_III_vs_Meitantei_Conan__The_Movie\n",
      "Fetching description for: - https://myanimelist.net/anime/42745/Machikado_Mazoku__2-choume\n",
      "Fetching description for: - https://myanimelist.net/anime/35798/Meitantei_Conan_Movie_22__Zero_no_Shikkounin\n",
      "Fetching description for: - https://myanimelist.net/anime/54724/Nige_Jouzu_no_Wakagimi\n",
      "Fetching description for: - https://myanimelist.net/anime/795/Oniisama_e\n",
      "Fetching description for: - https://myanimelist.net/anime/2605/Sayonara_Zetsubou_Sensei\n",
      "Fetching description for: - https://myanimelist.net/anime/26123/Seitokai_Yakuindomo_OVA\n",
      "Fetching description for: - https://myanimelist.net/anime/6862/K-On__Live_House\n",
      "Fetching description for: - https://myanimelist.net/anime/429/Kaleido_Star__Legend_of_Phoenix_-_Layla_Hamilton_Monogatari\n",
      "Fetching description for: - https://myanimelist.net/anime/48926/Komi-san_wa_Comyushou_desu\n",
      "Fetching description for: - https://myanimelist.net/anime/31741/Magi__Sinbad_no_Bouken_TV\n",
      "Fetching description for: - https://myanimelist.net/anime/569/Musekinin_Kanchou_Tylor\n",
      "Fetching description for: - https://myanimelist.net/anime/30885/Noragami_Aragoto_OVA\n",
      "Fetching description for: - https://myanimelist.net/anime/25161/One_Piece_3D2Y__Ace_no_shi_wo_Koete_Luffy_Nakama_Tono_Chikai\n",
      "Fetching description for: - https://myanimelist.net/anime/36796/Owarimonogatari_2nd_Season_Recaps\n",
      "Fetching description for: - https://myanimelist.net/anime/20371/Sekaiichi_Hatsukoi_Movie__Yokozawa_Takafumi_no_Baai\n",
      "Fetching description for: - https://myanimelist.net/anime/6505/Tteotda_Keunyeo\n",
      "Fetching description for: - https://myanimelist.net/anime/1921/Urusei_Yatsura_2__Beautiful_Dreamer\n",
      "Fetching description for: - https://myanimelist.net/anime/47405/Wanmei_Shijie\n",
      "Fetching description for: - https://myanimelist.net/anime/6586/Yume-iro_PÃ¢tissiÃ¨re\n",
      "Fetching description for: - https://myanimelist.net/anime/15039/Ano_Hi_Mita_Hana_no_Namae_wo_Bokutachi_wa_Mada_Shiranai_Movie\n",
      "Fetching description for: - https://myanimelist.net/anime/9513/Beelzebub\n",
      "Fetching description for: - https://myanimelist.net/anime/52505/Dark_Gathering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching description for: - https://myanimelist.net/anime/8408/Durarara_Specials\n",
      "Fetching description for: - https://myanimelist.net/anime/24471/Hoozuki_no_Reitetsu_OVA\n",
      "Fetching description for: - https://myanimelist.net/anime/48548/5-toubun_no_Hanayome_Movie\n",
      "Fetching description for: - https://myanimelist.net/anime/32998/91_Days\n",
      "Fetching description for: - https://myanimelist.net/anime/49785/Fairy_Tail__100-nen_Quest\n",
      "Fetching description for: - https://myanimelist.net/anime/40262/Haikyuu_Riku_vs_Kuu\n",
      "Fetching description for: - https://myanimelist.net/anime/14175/Hanasaku_Iroha_Movie__Home_Sweet_Home\n",
      "Fetching description for: - https://myanimelist.net/anime/3604/Hidamari_Sketch_x_365\n",
      "Fetching description for: - https://myanimelist.net/anime/36593/Hug_tto_Precure\n",
      "Fetching description for: - https://myanimelist.net/anime/39534/Jibaku_Shounen_Hanako-kun\n"
     ]
    }
   ],
   "source": [
    "for link in links:\n",
    "        print(f\"Fetching description for: - {link}\")\n",
    "\n",
    "        # Send request to the specific anime page\n",
    "        response1 = requests.get(link).text\n",
    "        soup1 = BeautifulSoup(response1, \"html.parser\")\n",
    "\n",
    "        # Try to find the description\n",
    "        description_tag = soup1.find(\"p\", itemprop=\"description\")\n",
    "        \n",
    "        english = soup1.find(\"div\",  class_=\"js-alternative-titles hide\")\n",
    "        if description_tag:\n",
    "            description = description_tag.text.strip()\n",
    "            descriptions.append(description)\n",
    "        else:\n",
    "            descriptions.append(\"NAN\")\n",
    "        if english:\n",
    "            english_t = english.find(\"span\", class_=\"dark_text\").find_next_sibling(text=True)\n",
    "            if english_t:\n",
    "                english_title.append(english_t)\n",
    "        else:\n",
    "            english_title.append(\"NAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(anime_names))\n",
    "print(len(scores))\n",
    "print(len(english_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.DataFrame({\n",
    "    \"Title\": anime_names, \n",
    "    \"anime_details\": anime_details,\n",
    "    \"score\": scores,\n",
    "    \"description\" : descriptions,\n",
    "    \"English_titles\" : english_title\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>anime_details</th>\n",
       "      <th>score</th>\n",
       "      <th>description</th>\n",
       "      <th>English_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JoJo no Kimyou na Bouken (TV)</td>\n",
       "      <td>TV (26 eps)\\n\\n        Oct 2012 - Apr 2013\\n\\n...</td>\n",
       "      <td>7.88</td>\n",
       "      <td>The year is 1868; English nobleman George Joes...</td>\n",
       "      <td>JoJo's Bizarre Adventure (2012)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Macross F</td>\n",
       "      <td>TV (25 eps)\\n\\n        Apr 2008 - Sep 2008\\n\\n...</td>\n",
       "      <td>7.88</td>\n",
       "      <td>Following a catastrophic war against a race of...</td>\n",
       "      <td>Macross Frontier\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megalo Box</td>\n",
       "      <td>TV (13 eps)\\n\\n        Apr 2018 - Jun 2018\\n\\n...</td>\n",
       "      <td>7.88</td>\n",
       "      <td>\"To be quiet and do as you're told, that's the...</td>\n",
       "      <td>Megalobox\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meitantei Conan Movie 02: 14-banme no Target</td>\n",
       "      <td>Movie (1 eps)\\n\\n        Apr 1998 - Apr 1998\\n...</td>\n",
       "      <td>7.88</td>\n",
       "      <td>A mysterious attacker has appeared and is assa...</td>\n",
       "      <td>Case Closed Movie 02: The Fourteenth Target\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One Piece Film: Gold</td>\n",
       "      <td>Movie (1 eps)\\n\\n        Jul 2016 - Jul 2016\\n...</td>\n",
       "      <td>7.88</td>\n",
       "      <td>Monkey D. Luffy and his Straw Hat Crew have fi...</td>\n",
       "      <td>One Piece Film 13: Gold\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Haikyuu!! Riku vs. Kuu</td>\n",
       "      <td>OVA (2 eps)\\n\\n        Jan 2020 - Jan 2020\\n\\n...</td>\n",
       "      <td>7.83</td>\n",
       "      <td>An intense battle rages on at the Tokyo Qualif...</td>\n",
       "      <td>Haikyu!! Land vs. Air\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Hanasaku Iroha Movie: Home Sweet Home</td>\n",
       "      <td>Movie (1 eps)\\n\\n        Mar 2013 - Mar 2013\\n...</td>\n",
       "      <td>7.83</td>\n",
       "      <td>Ohana Matsumae has been working at Kissui Inn ...</td>\n",
       "      <td>Hanasaku Iroha the Movie: Home Sweet Home\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hidamari Sketch x 365</td>\n",
       "      <td>TV (13 eps)\\n\\n        Jul 2008 - Sep 2008\\n\\n...</td>\n",
       "      <td>7.83</td>\n",
       "      <td>After passing her entrance exam, Yuno enrolls ...</td>\n",
       "      <td>Hidamari Sketch x 365\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Hug tto! Precure</td>\n",
       "      <td>TV (49 eps)\\n\\n        Feb 2018 - Jan 2019\\n\\n...</td>\n",
       "      <td>7.83</td>\n",
       "      <td>It's her first day at a new school, and the ch...</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jibaku Shounen Hanako-kun</td>\n",
       "      <td>TV (12 eps)\\n\\n        Jan 2020 - Mar 2020\\n\\n...</td>\n",
       "      <td>7.83</td>\n",
       "      <td>The famous Seven Mysteries that every school s...</td>\n",
       "      <td>Toilet-Bound Hanako-kun\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Title  \\\n",
       "0                  JoJo no Kimyou na Bouken (TV)   \n",
       "1                                      Macross F   \n",
       "2                                     Megalo Box   \n",
       "3   Meitantei Conan Movie 02: 14-banme no Target   \n",
       "4                           One Piece Film: Gold   \n",
       "..                                           ...   \n",
       "95                        Haikyuu!! Riku vs. Kuu   \n",
       "96         Hanasaku Iroha Movie: Home Sweet Home   \n",
       "97                         Hidamari Sketch x 365   \n",
       "98                              Hug tto! Precure   \n",
       "99                     Jibaku Shounen Hanako-kun   \n",
       "\n",
       "                                        anime_details score  \\\n",
       "0   TV (26 eps)\\n\\n        Oct 2012 - Apr 2013\\n\\n...  7.88   \n",
       "1   TV (25 eps)\\n\\n        Apr 2008 - Sep 2008\\n\\n...  7.88   \n",
       "2   TV (13 eps)\\n\\n        Apr 2018 - Jun 2018\\n\\n...  7.88   \n",
       "3   Movie (1 eps)\\n\\n        Apr 1998 - Apr 1998\\n...  7.88   \n",
       "4   Movie (1 eps)\\n\\n        Jul 2016 - Jul 2016\\n...  7.88   \n",
       "..                                                ...   ...   \n",
       "95  OVA (2 eps)\\n\\n        Jan 2020 - Jan 2020\\n\\n...  7.83   \n",
       "96  Movie (1 eps)\\n\\n        Mar 2013 - Mar 2013\\n...  7.83   \n",
       "97  TV (13 eps)\\n\\n        Jul 2008 - Sep 2008\\n\\n...  7.83   \n",
       "98  TV (49 eps)\\n\\n        Feb 2018 - Jan 2019\\n\\n...  7.83   \n",
       "99  TV (12 eps)\\n\\n        Jan 2020 - Mar 2020\\n\\n...  7.83   \n",
       "\n",
       "                                          description  \\\n",
       "0   The year is 1868; English nobleman George Joes...   \n",
       "1   Following a catastrophic war against a race of...   \n",
       "2   \"To be quiet and do as you're told, that's the...   \n",
       "3   A mysterious attacker has appeared and is assa...   \n",
       "4   Monkey D. Luffy and his Straw Hat Crew have fi...   \n",
       "..                                                ...   \n",
       "95  An intense battle rages on at the Tokyo Qualif...   \n",
       "96  Ohana Matsumae has been working at Kissui Inn ...   \n",
       "97  After passing her entrance exam, Yuno enrolls ...   \n",
       "98  It's her first day at a new school, and the ch...   \n",
       "99  The famous Seven Mysteries that every school s...   \n",
       "\n",
       "                                       English_titles  \n",
       "0             JoJo's Bizarre Adventure (2012)\\n        \n",
       "1                            Macross Frontier\\n        \n",
       "2                                   Megalobox\\n        \n",
       "3    Case Closed Movie 02: The Fourteenth Target\\n...  \n",
       "4                       One Piece Film 13: Gold\\n      \n",
       "..                                                ...  \n",
       "95                      Haikyu!! Land vs. Air\\n        \n",
       "96   Hanasaku Iroha the Movie: Home Sweet Home\\n  ...  \n",
       "97                      Hidamari Sketch x 365\\n        \n",
       "98                                                NAN  \n",
       "99                    Toilet-Bound Hanako-kun\\n        \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime[\"description\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(anime)\n",
    "\n",
    "# Export DataFrame to CSV file\n",
    "anime.to_csv('anime15_list.csv', index=False)  # Set index=False to exclude row indices from the CSV file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "link = \"https://www.sec.gov/Archives/edgar/data/4281/000119312513062916/R2.htm\"\n",
    "\n",
    "request_headers = {\"Accept-Language\": \"en-US,en;q=0.5\", \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Referer\": \"http://google.com\", \"Connection\": \"keep-alive\"}\n",
    "reuest = requests.get(link, headers=request_headers)\n",
    "soup = BeautifulSoup(reuest.text,\"lxml\")\n",
    "print(soup.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\91971\\anime2_list.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keyword = 'Sousou no Frieren'\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Path to your ChromeDriver\n",
    "chrome_driver_path = r\"C:\\Users\\91971\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "# Create a Service instance with the path to the ChromeDriver\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "\n",
    "# Initialize WebDriver with the service instance\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Navigate to a website\n",
    "driver.get('https://www.google.com/')\n",
    "search_query = driver.find_element(By.CLASS_NAME, \" gLFyf\")\n",
    "search_query.send_keys(keyword + ' wikipedia')\n",
    "search_query.send_keys(Keys.RETURN)\n",
    "\n",
    "wiki_url = driver.find_element_by_class_name('iUh30').text.replace(' â€º ','/')\n",
    "driver.get(wiki_url)\n",
    "\n",
    "text = driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div[1]/p[2]').text\n",
    "if text=='':\n",
    "    text = driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div[1]/p[3]').text\n",
    "text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "\n",
    "keyword = 'Sousou no Frieren'\n",
    "\n",
    "chrome_driver_path = r\"C:\\Users\\91971\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "\n",
    "driver.get('https://www.google.com/')\n",
    "\n",
    "# Find the search box and enter the search query\n",
    "search_query = driver.find_element(By.NAME, \"q\")\n",
    "search_query.send_keys(keyword + ' wikipedia')\n",
    "search_query.send_keys(Keys.RETURN)\n",
    "\n",
    "driver.implicitly_wait(3)  \n",
    "\n",
    "\n",
    "wiki_url = driver.find_element(By.CSS_SELECTOR, 'a[href*=\"wikipedia.org\"]').get_attribute('href')\n",
    "driver.get(wiki_url)\n",
    "\n",
    "# Extract text from the target Wikipedia page paragraph\n",
    "try:\n",
    "    \n",
    "    text = driver.find_element(By.XPATH, '//*[@id=\"mw-heading mw-heading2\"]//p[1]').text\n",
    "    if text == '':\n",
    "        text = driver.find_element(By.XPATH, '//*[@id= \"mw-heading mw-heading2\"/div[1]/p[1]').text\n",
    "        \n",
    "    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)  # Remove content inside brackets\n",
    "    print(text)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "chrome_driver_path = r\"C:\\Users\\91971\\chromedriver-win64\\chromedriver.exe\"\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Navigate to the Wikipedia page (replace 'Sousou_no_Frieren' with your anime title in the URL)\n",
    "driver.get(\"https://en.wikipedia.org/wiki/Sousou_no_Frieren\")\n",
    "\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "# Locate the \"Plot\" heading and retrieve the next three paragraphs\n",
    "try:\n",
    "    # Find the \"Plot\" section header div and the next paragraphs\n",
    "    plot_header = driver.find_element(By.XPATH, '//h2[@id=\"Plot\"]/ancestor::div[contains(@class, \"mw-heading\")]')\n",
    "    \n",
    "    # Get the next three <p> tags after the \"Plot\" header\n",
    "    plot_paragraphs = plot_header.find_elements(By.XPATH, 'following-sibling::p[position() <= 3]')\n",
    "\n",
    "    # Extract and clean the text from each paragraph\n",
    "    plot_text = \"\"\n",
    "    for paragraph in plot_paragraphs:\n",
    "        plot_text += paragraph.text + \"\\n\"\n",
    "\n",
    "    # Clean up text by removing citations and extra brackets\n",
    "    plot_text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", plot_text)\n",
    "    print(plot_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Plot section not found:\", e)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\91971\\Downloads\\chromedriver.exe\",options=options)\n",
    "driver.get('https://www.google.com/')\n",
    "\n",
    "search_query = driver.find_element_by_name('q')\n",
    "search_query.send_keys(keyword + ' wikipedia')\n",
    "search_query.send_keys(Keys.RETURN)\n",
    "\n",
    "wiki_url = driver.find_element_by_class_name('iUh30').text.replace(' â€º ','/')\n",
    "driver.get(wiki_url)\n",
    "\n",
    "text = driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div[1]/p[2]').text\n",
    "if text=='':\n",
    "    text = driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div[1]/p[3]').text\n",
    "text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "\n",
    "def get_anime_description(anime_name):\n",
    "    \"\"\"\n",
    "    Get the paragraphs of the Wikipedia page for an anime that come after a specific div.\n",
    "    \n",
    "    Args:\n",
    "        anime_name (str): Name of the anime to search for\n",
    "    \n",
    "    Returns:\n",
    "        str: Combined paragraphs after the specified div or error message\n",
    "    \"\"\"\n",
    "    \n",
    "    # Headers to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    " \n",
    "\n",
    "    # Navigate to the Wikipedia page (replace 'Sousou_no_Frieren' with your anime title in the URL)\n",
    "    driver.get(f\"https://en.wikipedia.org/wiki/{anime_name}\")\n",
    "    driver.implicitly_wait(3)\n",
    "    # Locate the \"Plot\" heading and retrieve the next three paragraphs\n",
    "    try:\n",
    "        # Find the \"Plot\" section header div and the next paragraphs\n",
    "        plot_header = driver.find_element(By.XPATH, '//h2[@id=\"Plot\"]/ancestor::div[contains(@class, \"mw-heading\")]')\n",
    "    \n",
    "        # Get the next three <p> tags after the \"Plot\" header\n",
    "        plot_paragraphs = plot_header.find_elements(By.XPATH, 'following-sibling::p[position() <= 3]')\n",
    "\n",
    "        # Extract and clean the text from each paragraph\n",
    "        plot_text = \"\"\n",
    "        for paragraph in plot_paragraphs:\n",
    "            plot_text += paragraph.text + \"\\n\"\n",
    "\n",
    "        # Clean up text by removing citations and extra brackets\n",
    "        plot_text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", plot_text)\n",
    "        print(plot_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Plot section not found:\", e)\n",
    "# Example usage:\n",
    "# anime_name = \"Great Teacher Onizuka\"  # Replace with your anime name\n",
    "# description = get_anime_description(anime_name)\n",
    "# print(description)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "chrome_driver_path = r\"C:\\Users\\91971\\chromedriver-win64\\chromedriver.exe\"\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\91971\\anime_list.csv\")\n",
    "df1 = df.head(1)\n",
    "results = get_anime_description(df1['Title'])\n",
    "results\n",
    "#results.to_csv('anime_with_wiki_info.csv', index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "import time\n",
    "\n",
    "def clean_anime_title(title):\n",
    "    \"\"\"Clean anime title for Wikipedia URL\"\"\"\n",
    "    # Handle special cases\n",
    "    special_cases = {\n",
    "        \"GintamaÂ°\": \"Gintama\",\n",
    "        \"Gintama'\": \"Gintama\",\n",
    "        \"Gintama': Enchousen\": \"Gintama\",\n",
    "        \"Steins;Gate\": \"Steins;_Gate\",\n",
    "        \"Hunter x Hunter (2011)\": \"Hunter_Ã—_Hunter_(2011_TV_series)\",\n",
    "        \"Shingeki no Kyojin Season 3 Part 2\": \"Attack_on_Titan_(TV_series)\"\n",
    "    }\n",
    "    \n",
    "    if title in special_cases:\n",
    "        return special_cases[title]\n",
    "    \n",
    "    # General cleaning\n",
    "    clean_title = title.replace(' ', '_')\n",
    "    clean_title = quote(clean_title)\n",
    "    return clean_title\n",
    "\n",
    "def get_anime_description(driver, anime_title):\n",
    "    \"\"\"Get the plot description from Wikipedia with improved element detection\"\"\"\n",
    "    try:\n",
    "        # Format the URL with cleaned title\n",
    "        formatted_title = clean_anime_title(anime_title)\n",
    "        url = f\"https://en.wikipedia.org/wiki/{formatted_title}\"\n",
    "        print(f\"Trying URL: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "        \n",
    "        # Try multiple possible section names and structures\n",
    "        section_names = [\"Plot\", \"Story\", \"Synopsis\", \"Premise\", \"Overview\"]\n",
    "        plot_section = None\n",
    "        \n",
    "        # First try: Look for sections\n",
    "        for section_name in section_names:\n",
    "            xpath_patterns = [\n",
    "                f'//span[@id=\"{section_name}\"]//ancestor::h2',\n",
    "                f'//h2[.//span[@class=\"mw-headline\" and contains(text(),\"{section_name}\")]]',\n",
    "                f'//h3[.//span[@class=\"mw-headline\" and contains(text(),\"{section_name}\")]]'\n",
    "            ]\n",
    "            \n",
    "            for xpath in xpath_patterns:\n",
    "                try:\n",
    "                    plot_section = driver.find_element(By.XPATH, xpath)\n",
    "                    print(f\"Found section using: {xpath}\")\n",
    "                    break\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "            \n",
    "            if plot_section:\n",
    "                break\n",
    "        \n",
    "        # If no section found, try getting first few paragraphs\n",
    "        if not plot_section:\n",
    "            print(f\"No plot section found for {anime_title}, trying first paragraphs\")\n",
    "            try:\n",
    "                content_div = driver.find_element(By.ID, \"mw-content-text\")\n",
    "                first_paras = content_div.find_elements(By.XPATH, './/p[string-length(normalize-space()) > 50][position() <= 3]')\n",
    "                if first_paras:\n",
    "                    plot_text = \"\\n\".join([p.text for p in first_paras])\n",
    "                    plot_text = clean_plot_text(plot_text)\n",
    "                    return plot_text if plot_text.strip() else \"No suitable content found\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting first paragraphs: {e}\")\n",
    "                return \"No plot information found\"\n",
    "        \n",
    "        # Get paragraphs after the plot section\n",
    "        plot_paragraphs = []\n",
    "        current = plot_section\n",
    "        para_count = 0\n",
    "        \n",
    "        while para_count < 3:\n",
    "            try:\n",
    "                next_paras = current.find_elements(By.XPATH, 'following-sibling::p')\n",
    "                for para in next_paras:\n",
    "                    if len(para.text.strip()) > 50:  # Only include substantial paragraphs\n",
    "                        plot_paragraphs.append(para.text)\n",
    "                        para_count += 1\n",
    "                        if para_count >= 3:\n",
    "                            break\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "        \n",
    "        if not plot_paragraphs:\n",
    "            return \"No plot paragraphs found\"\n",
    "        \n",
    "        plot_text = \"\\n\".join(plot_paragraphs)\n",
    "        return clean_plot_text(plot_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {anime_title}: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def clean_plot_text(text):\n",
    "    \"\"\"Clean up the plot text by removing citations and normalizing whitespace\"\"\"\n",
    "    # Remove citations\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    # Remove parenthetical citations\n",
    "    text = re.sub(r'\\([^)]*?\\[\\d+\\][^)]*?\\)', '', text)\n",
    "    # Remove other brackets content\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def process_anime_list(csv_path, chrome_driver_path):\n",
    "    \"\"\"Process a CSV file of anime titles and get their plot descriptions\"\"\"\n",
    "    service = Service(executable_path=chrome_driver_path)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df1= df.head()\n",
    "        results = []\n",
    "        \n",
    "        for index, row in df1.iterrows():\n",
    "            title = row['Title']\n",
    "            print(f\"\\nProcessing: {title}\")\n",
    "            plot = get_anime_description(driver, title)\n",
    "            \n",
    "            results.append({\n",
    "                'Title': title,\n",
    "                'Plot': plot,\n",
    "                'URL': f\"https://en.wikipedia.org/wiki/{clean_anime_title(title)}\"\n",
    "            })\n",
    "            \n",
    "            # Save progress after each anime\n",
    "            temp_df = pd.DataFrame(results)\n",
    "            temp_df.to_csv('anime_with_wiki_info_progress.csv', index=False)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chrome_driver_path = r\"C:\\Users\\91971\\chromedriver-win64\\chromedriver.exe\"\n",
    "    input_csv = r\"C:\\Users\\91971\\anime_list.csv\"\n",
    "    \n",
    "    results_df = process_anime_list(input_csv, chrome_driver_path)\n",
    "    #results_df.to_csv('anime_with_wiki_info.csv', index=False)\n",
    "    results_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def get_anime_description(driver, anime_title):\n",
    "    \"\"\"\n",
    "    Get the plot description from Wikipedia with improved element detection\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        anime_title (str): Name of the anime\n",
    "    \n",
    "    Returns:\n",
    "        str: Plot description or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format the URL - replace spaces with underscores\n",
    "        formatted_title = anime_title.replace(' ', '_')\n",
    "        url = f\"https://en.wikipedia.org/wiki/{formatted_title}\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "        \n",
    "        plot_section = None\n",
    "        xpath_patterns = [\n",
    "            # Try finding by Plot heading ID\n",
    "            '//h2[@id=\"Plot\"]',\n",
    "            # Try finding by heading with Plot text\n",
    "            '//h2[span[@class=\"mw-headline\" and text()=\"Plot\"]]',\n",
    "            # Try finding by Story heading (some anime pages use \"Story\" instead of \"Plot\")\n",
    "            '//h2[@id=\"Story\"]',\n",
    "            '//h2[span[@class=\"mw-headline\" and text()=\"Story\"]]',\n",
    "            # Try finding by Synopsis heading\n",
    "            '//h2[@id=\"Synopsis\"]',\n",
    "            '//h2[span[@class=\"mw-headline\" and text()=\"Synopsis\"]]',\n",
    "        ]\n",
    "        \n",
    "        for xpath in xpath_patterns:\n",
    "            try:\n",
    "                plot_section = driver.find_element(By.XPATH, xpath)\n",
    "                print(f\"Found plot section using pattern: {xpath}\")\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        \n",
    "        if not plot_section:\n",
    "            return \"Plot section not found. The page might have a different structure.\"\n",
    "        \n",
    "    \n",
    "       \n",
    "        plot_paragraphs = plot_section.find_elements(By.XPATH, 'following-sibling::p[position() <= 3]')\n",
    "        \n",
    "       \n",
    "                        \n",
    "\n",
    "        # Extract and clean the text from each paragraph\n",
    "        plot_text = \"\"\n",
    "        for paragraph in plot_paragraphs:\n",
    "            plot_text += paragraph.text + \"\\n\"\n",
    "\n",
    "        # Clean up text by removing citations and extra brackets\n",
    "        plot_text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", plot_text)\n",
    "        print(plot_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Plot section not found:\", e)\n",
    "        \n",
    "        \n",
    "\n",
    "def process_anime_list(csv_path, chrome_driver_path):\n",
    "    \"\"\"Process a CSV file of anime titles and get their plot descriptions\"\"\"\n",
    "    # Set up the Chrome driver\n",
    "    \n",
    "    service = Service(executable_path=chrome_driver_path)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df1 = df.head(10)\n",
    "        results = []\n",
    "        \n",
    "        # Process each anime\n",
    "        for index, row in df1.iterrows():\n",
    "            title = row['Title']\n",
    "            print(f\"Processing: {title}\")\n",
    "            plot = get_anime_description(driver, title)\n",
    "            results.append({\n",
    "                'Title': title,\n",
    "                'Plot': plot\n",
    "            })\n",
    "            time.sleep(2)  # Be nice to Wikipedia's servers\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    chrome_driver_path = r\"C:\\Users\\91971\\chromedriver-win64\\chromedriver.exe\"\n",
    "    input_csv = r\"C:\\Users\\91971\\anime_list.csv\"\n",
    "    \n",
    "    results_df = process_anime_list(input_csv, chrome_driver_path)\n",
    "    results_df.to_csv('anime_with_wiki_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.generativeai\n",
      "  Downloading google_generativeai-0.1.0rc1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-ai-generativelanguage==0.1.0 (from google.generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.1.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai)\n",
      "  Downloading google_api_core-2.22.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0 (from google-ai-generativelanguage==0.1.0->google.generativeai)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\users\\91971\\appdata\\roaming\\python\\python38\\site-packages (from google-ai-generativelanguage==0.1.0->google.generativeai) (3.20.3)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (2.17.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (2.24.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (1.53.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai)\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (4.9)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-ai-generativelanguage==0.1.0->google.generativeai)\n",
      "  Downloading protobuf-4.25.5-cp38-cp38-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai)\n",
      "  Downloading grpcio-1.67.1-cp38-cp38-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google.generativeai) (0.4.8)\n",
      "Downloading google_generativeai-0.1.0rc1-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 117.3/117.3 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading google_ai_generativelanguage-0.1.0-py3-none-any.whl (109 kB)\n",
      "   ---------------------------------------- 109.3/109.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.22.0-py3-none-any.whl (156 kB)\n",
      "   ---------------------------------------- 156.5/156.5 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 50.1/50.1 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 220.9/220.9 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-4.25.5-cp38-cp38-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 413.4/413.4 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.67.1-cp38-cp38-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 4.4/4.4 MB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, grpcio, proto-plus, googleapis-common-protos, grpcio-status, google-api-core, google-ai-generativelanguage, google.generativeai"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.0 requires protobuf<4,>=3.11, but you have protobuf 4.25.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.53.0\n",
      "    Uninstalling grpcio-1.53.0:\n",
      "      Successfully uninstalled grpcio-1.53.0\n",
      "Successfully installed google-ai-generativelanguage-0.1.0 google-api-core-2.22.0 google.generativeai-0.1.0rc1 googleapis-common-protos-1.65.0 grpcio-1.67.1 grpcio-status-1.62.3 proto-plus-1.25.0 protobuf-4.22.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\91971\\anaconda3\\lib\\site-packages (0.1.0rc1)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.1.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-generativeai) (0.1.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (2.22.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.1.0->google-generativeai) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.1.0->google-generativeai) (4.22.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (1.65.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (2.17.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (2.24.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (1.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.1.0->google-generativeai) (0.4.8)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Sousou no Frieren: 404 Requested entity was not found.\n",
      "Error processing One Piece Fan Letter: 404 Requested entity was not found.\n",
      "Error processing Fullmetal Alchemist: Brotherhood: 404 Requested entity was not found.\n",
      "Error processing Steins;Gate: 404 Requested entity was not found.\n",
      "Error processing Shingeki no Kyojin Season 3 Part 2: 404 Requested entity was not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 of 5 titles\n",
      "Processing complete! Results saved to anime_descriptions_gemini.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
